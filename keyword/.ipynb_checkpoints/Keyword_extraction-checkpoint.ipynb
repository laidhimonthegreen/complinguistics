{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). Датасет НГ самый маленький, поэтому возьмем его в качестве примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачаем данные в папке data и распакуем их\n",
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим файлы в один датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True) for file in files][:1], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В среду состоялось отложенное заседание Совета по федеральным государственным образовательным стандартам (ФГОС) при Министерстве образования и науки РФ. Собрание должно было состояться еще в понедельник, но было перенесено по просьбе членов совета. И вот пришло сообщение, что общественники выразили согласие с позицией министерства. Новые ФГОСы приняты.\\nНа вчерашнем заседании был принят ФГОС по начальной общеобразовательной школе. До 28 марта продлятся косультации по ФГОСам для средней школы.\\nНапомним, что накануне Гильдия словесников разместила открытое письмо на имя министра образования и науки РФ Ольги Васильевой. По мнению авторов письма, новые ФГОСы грубо нарушают права детей, уже проучившихся по существующему стандарту до 6-го класса. Приняв новый стандарт, Министерство образования дает право контролирующим органам ловить детей на незнании большого списка произведений (235 за пять лет обучения). «Это исключает возможность полноценного их освоения, создает риск формального, п...</td>\n",
       "      <td>[школа, образовательные стандарты, литература, история, фгос]</td>\n",
       "      <td>Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы</td>\n",
       "      <td>Ольга Васильева обещала \"НГ\" не перегружать школьников</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хорошо, когда красота в глазах смотрящего живет свободно или хотя бы занимает широкий угол зрения. Плохо было б, если б она вовсе не озаряла своим светом космическую темень пустоты зрачка. Слава богу, такое вряд ли возможно. \\nА случается, что красота уходит. Почему вдруг? И куда она девается, когда в один из философских обходов своего организма вы, еще недавно гордый ее обладатель, обескураженно ее  недосчитываетесь? \\nВообразите: прелестнейшее из созданий – ваша кошка пластичнейшими движениями рвет банкноту за банкнотой, забирается на карниз по шелковой занавеске или отгрызает полпаспорта. Где, скажите, теперь красота этой кошки? Или другой пример – с зазнобой сердца. Предмет романтичнейших грез наконец-то садится с вами на заветную скамейку в парке – закат, пение птах… И тут он силой своего обаяния с оглушительным плюхом обрушивает вокруг вас красоту и гармонию столетних дубов, тополей и прочего. Где, спрашивается, красота момента? \\nЕсли от сказки после того, как ее рассказали,...</td>\n",
       "      <td>[красота, законы]</td>\n",
       "      <td>О живительной пользе укорота при выборе между плохим и хорошим</td>\n",
       "      <td>У красоты собственные закон и воля</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/style/2018-03-19/8_7192_beauty.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Когда-то Леонид Юзефович написал книгу о монгольской эпопее барона Унгерна «Самодержец пустыни» – она стала интеллектуальным бестселлером и классикой жанра – документальный роман. В то время автор попутно изучал и историю вооруженного восстания в Якутии в 1922–1923 годах под руководством Анатолия Пепеляева. И вот теперь из «якутского» материала сложилась отдельная книга. Тема ее для нынешнего читателя поистине раритетна. Ведь воевавший где-то на самом краю страны Пепеляев практически забыт, притом что о борьбе с ним когда-то в СССР выходили статьи и книги. В памяти потомков, образно говоря, от Пепеляева остался только пепел.\\nЮзефович воскрешает в памяти не только его военные дела, но и человеческие черты. Этот провинциальный интеллигент, неврастеник и фаталист, начал восстание, практически не имея шансов на успех. Однако силою недюжинной харизмы Пепеляев сумел собрать вокруг себя многих боевых офицеров, таежных охотников и недовольных новыми порядками аборигенов. Для своих 32 лет ...</td>\n",
       "      <td>[юзефович, гражданская война, пепеляев, якутия]</td>\n",
       "      <td>Крепость из тел и призрак независимой Якутии</td>\n",
       "      <td>Апокалиптический бунт</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/zavisimaya/2017-12-19/15_7139_bunt.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   content  \\\n",
       "0  В среду состоялось отложенное заседание Совета по федеральным государственным образовательным стандартам (ФГОС) при Министерстве образования и науки РФ. Собрание должно было состояться еще в понедельник, но было перенесено по просьбе членов совета. И вот пришло сообщение, что общественники выразили согласие с позицией министерства. Новые ФГОСы приняты.\\nНа вчерашнем заседании был принят ФГОС по начальной общеобразовательной школе. До 28 марта продлятся косультации по ФГОСам для средней школы.\\nНапомним, что накануне Гильдия словесников разместила открытое письмо на имя министра образования и науки РФ Ольги Васильевой. По мнению авторов письма, новые ФГОСы грубо нарушают права детей, уже проучившихся по существующему стандарту до 6-го класса. Приняв новый стандарт, Министерство образования дает право контролирующим органам ловить детей на незнании большого списка произведений (235 за пять лет обучения). «Это исключает возможность полноценного их освоения, создает риск формального, п...   \n",
       "1  Хорошо, когда красота в глазах смотрящего живет свободно или хотя бы занимает широкий угол зрения. Плохо было б, если б она вовсе не озаряла своим светом космическую темень пустоты зрачка. Слава богу, такое вряд ли возможно. \\nА случается, что красота уходит. Почему вдруг? И куда она девается, когда в один из философских обходов своего организма вы, еще недавно гордый ее обладатель, обескураженно ее  недосчитываетесь? \\nВообразите: прелестнейшее из созданий – ваша кошка пластичнейшими движениями рвет банкноту за банкнотой, забирается на карниз по шелковой занавеске или отгрызает полпаспорта. Где, скажите, теперь красота этой кошки? Или другой пример – с зазнобой сердца. Предмет романтичнейших грез наконец-то садится с вами на заветную скамейку в парке – закат, пение птах… И тут он силой своего обаяния с оглушительным плюхом обрушивает вокруг вас красоту и гармонию столетних дубов, тополей и прочего. Где, спрашивается, красота момента? \\nЕсли от сказки после того, как ее рассказали,...   \n",
       "2  Когда-то Леонид Юзефович написал книгу о монгольской эпопее барона Унгерна «Самодержец пустыни» – она стала интеллектуальным бестселлером и классикой жанра – документальный роман. В то время автор попутно изучал и историю вооруженного восстания в Якутии в 1922–1923 годах под руководством Анатолия Пепеляева. И вот теперь из «якутского» материала сложилась отдельная книга. Тема ее для нынешнего читателя поистине раритетна. Ведь воевавший где-то на самом краю страны Пепеляев практически забыт, притом что о борьбе с ним когда-то в СССР выходили статьи и книги. В памяти потомков, образно говоря, от Пепеляева остался только пепел.\\nЮзефович воскрешает в памяти не только его военные дела, но и человеческие черты. Этот провинциальный интеллигент, неврастеник и фаталист, начал восстание, практически не имея шансов на успех. Однако силою недюжинной харизмы Пепеляев сумел собрать вокруг себя многих боевых офицеров, таежных охотников и недовольных новыми порядками аборигенов. Для своих 32 лет ...   \n",
       "\n",
       "                                                        keywords  \\\n",
       "0  [школа, образовательные стандарты, литература, история, фгос]   \n",
       "1                                              [красота, законы]   \n",
       "2                [юзефович, гражданская война, пепеляев, якутия]   \n",
       "\n",
       "                                                                                                                                 summary  \\\n",
       "0  Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы   \n",
       "1                                                                         О живительной пользе укорота при выборе между плохим и хорошим   \n",
       "2                                                                                           Крепость из тел и призрак независимой Якутии   \n",
       "\n",
       "                                                    title  \\\n",
       "0  Ольга Васильева обещала \"НГ\" не перегружать школьников   \n",
       "1                      У красоты собственные закон и воля   \n",
       "2                                   Апокалиптический бунт   \n",
       "\n",
       "                                                                             url  \n",
       "0  https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html  \n",
       "1      https://amp.ng.ru/?p=http://www.ng.ru/style/2018-03-19/8_7192_beauty.html  \n",
       "2  https://amp.ng.ru/?p=http://www.ng.ru/zavisimaya/2017-12-19/15_7139_bunt.html  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждой статье приписано какое-то количество ключевых слов. Допустим, что это единственно правильный набор ключевых слов (что конечно не так, но других данных у нас нет). Наша задача - придумать как извлекать точно такой же список автоматически.  \n",
    "Зададим несколько метрик, по которым будем определять качество извлекаемых ключевых слов - точность, полноту, ф1-меру и меру жаккарда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что всё работает как надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  1.0\n",
      "Recall -  1.0\n",
      "F1 -  1.0\n",
      "Jaccard -  1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тупое решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте не будем думать, а попробуем сразу придумать какое-то решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем первые 5 слов из заголовка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.06\n",
      "F1 -  0.06\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: x.lower().split()[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.06\n",
      "Recall -  0.07\n",
      "F1 -  0.06\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['title'].apply(lambda x: x.lower().split()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем взять самые частотные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.02\n",
      "Recall -  0.04\n",
      "F1 -  0.02\n",
      "Jaccard -  0.01\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content'].apply(lambda x: \n",
    "                                                 [x[0] for x in Counter(x.lower().split()).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или вообще рандомные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.01\n",
      "Recall -  0.01\n",
      "F1 -  0.01\n",
      "Jaccard -  0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content'].apply(lambda x: \n",
    "                                                 np.random.choice(list(set(x.lower().split())), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим, что вообще извлекается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [\"молодежное, \"яблоко\":, оппозиционная, деятельность, становится, опасной]\n",
       "1                                                                 [\"газпрома\", на, всех, не, хватит]\n",
       "2                                                   [бесконечная, партия, в, четырехмерные, шахматы]\n",
       "3    [экс-депутат,, осужденная, за, фальсификацию, выборов,, оказалась, членом, \"боевого, братства\"]\n",
       "4                               [новая, москва, останется, территорией, экологической, безопасности]\n",
       "5                                [f1., гран-при, сша, прошел, без, четырех, машин, и, со, «стопкой»]\n",
       "6                                          [100, ведущих, политиков, россии, в, феврале, 2018, года]\n",
       "7                                               [закон, \"о, культуре\", принимают, на, фоне, арестов]\n",
       "8                                    [насколько, реальна, газовая, подоплека, сирийского, конфликта]\n",
       "9                                  [фсб:, в, калужской, области, задержаны, четверо, участников, иг]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].apply(lambda x: x.lower().split()[:10]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      [в, и, на, не, что, –, его, «молодежное, с, это]\n",
       "1                                                            [в, и, на, –, млрд., куб., по, к, газа, м]\n",
       "2                                                                 [в, –, и, не, я, но, что, это, на, с]\n",
       "3                                                       [в, на, и, ким, по, –, что, видео, он, зинаиды]\n",
       "4                                              [в, и, на, новой, площадью, москвы, –, развития, с, для]\n",
       "5                                                             [в, на, и, не, с, но, уже, что, у, гонки]\n",
       "6                                                  [на, в, (с, место)., и, рф, позиции, влияние, по, с]\n",
       "7                                                        [в, и, –, по, с, культуре, не, из, будет, как]\n",
       "8                                                              [в, и, на, с, что, для, по, –, не, газа]\n",
       "9    [в, рф, террористической, организации, задержаны, –, четверо, участников, запрещенной, «исламское]\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'].apply(lambda x: [x[0] for x in Counter(x.lower().split()).most_common(10)]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда извлекаются частотные слова, то список почти полностью состоит из всяких стоп-слов. Также из-за плохой токенизации некоторые слова в обоих списках - пунктуация или слова с пунктуацией на концах. К тому же извлекаемые слова ненормализованы, а правильные ключевые слова - наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация, удаление стоп-слов и нормализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_norm'] = data['title'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_norm'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем те же самые методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.11\n",
      "Recall -  0.21\n",
      "F1 -  0.14\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "# топ 10 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.12\n",
      "F1 -  0.11\n",
      "Jaccard -  0.07\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'],data['title_norm'].apply(lambda x: x[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сильно улучшилось! Можно теперь ещё раз посмотреть, что плохого извлекается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [стандарт, который, источник, фгоса, васильев, ольга, исторический, предмет, результат, образовательный]\n",
       "1                                        [красота, ваш, отчаяние, глаз, это, уйти, порыв, кошка, значит, один]\n",
       "2                                 [пепеляев, юзеф, книга, якутия, война, самый, восстание, год, место, леонид]\n",
       "3                              [команда, гонка, это, пилот, сказать, mclaren, сезон, ferrari, время, mercedes]\n",
       "4                                          [есенин, поэт, клюев, год, свой, который, жизнь, это, смерть, 1925]\n",
       "5                      [наш, медицина, кафедра, это, медицинский, выпускник, работать, работа, уровень, рудна]\n",
       "6                                 [русский, книга, мозг, человек, два, говор, островной, это, анатомия, глава]\n",
       "7                         [ирак, война, американец, партизанский, это, структура, войско, единый, лишь, новый]\n",
       "8                      [который, нейросеть, ai, свой, клетка, самый, искусственный, интеллект, основа, задача]\n",
       "9                  [россия, вступление, кристалина, георгиев, вто, банка, страна, слово, директор, переговоры]\n",
       "10                                 [фильм, который, приз, картина, свой, кино, например, программа, один, год]\n",
       "11                                [это, сидеть, каша, который, столовый, человек, начинать, талон, рок, такой]\n",
       "12                            [место, псков, город, день, такой, андрей, туризм, князь, ганзейский, концепция]\n",
       "13                  [индонезия, который, страна, год, мусульманский, терроризм, это, человек, экстремизм, сша]\n",
       "14                                       [это, который, такой, свой, игра, каждый, стать, парень, самый, друг]\n",
       "15                                [выбор, путин, президент, признавать, лидер, страна, россия, рф, сша, голос]\n",
       "16                       [водный, год, вода, объект, загрязнение, качество, который, источник, это, хозяйство]\n",
       "17         [школа, образование, год, москва, школьник, международный, московский, олимпиада, высокий, который]\n",
       "18                                      [который, фильм, один, стать, слон, герой, это, содерберг, ван, новый]\n",
       "19                              [канада, один, швед, лундквист, ворота, счёт, свой, матч, период, олимпийский]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё остались некоторые стоп-слова. Вместо того, чтобы расширять список, давайте попробуем выкинуть несуществительные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.24\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещу улучшения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [стандарт, источник, предмет, васильев, результат, фгоса, ольга, школа, письмо, содержание]\n",
       "1                       [красота, отчаяние, глаз, порыв, кошка, место, руина, предмет, дело, вопрос]\n",
       "2                      [пепеляев, юзеф, книга, якутия, восстание, война, год, место, леонид, сибирь]\n",
       "3                       [команда, гонка, пилот, сезон, время, машина, круг, место, гран-при, борьба]\n",
       "4                      [есенин, поэт, клюев, год, жизнь, смерть, сергей, человек, борода, мариенгоф]\n",
       "5    [медицина, кафедра, выпускник, работа, уровень, рудна, год, практика, ординатор, преподаватель]\n",
       "6                 [книга, мозг, человек, говор, анатомия, звезда, глава, вопрос, азербайджан, слово]\n",
       "7                   [ирак, война, американец, структура, войско, хусейн, оружие, солдат, узел, штаб]\n",
       "8       [нейросеть, клетка, интеллект, задача, основа, время, изображение, работа, пиксель, человек]\n",
       "9        [россия, георгиев, кристалина, вступление, вто, банка, страна, директор, слово, переговоры]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень значимые слова все ещё остались. Давайте попробуем отсеять стоп-слова с помощью tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно заодно сделать нграммы\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем наши тексты в векторы, где на позиции i стоит tfidf коэффициент слова i из словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем векторы текстов по этим коэффициентам и возьмем топ-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортировка по убыванию, поэтому нужно развернуть список\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ольга васильев',\n",
       "  'стандарт',\n",
       "  'васильев',\n",
       "  'ольга',\n",
       "  'источник',\n",
       "  'предмет',\n",
       "  'письмо',\n",
       "  'произведение',\n",
       "  'редакция',\n",
       "  'содержание'],\n",
       " ['красота',\n",
       "  'отчаяние',\n",
       "  'порыв',\n",
       "  'кошка',\n",
       "  'глаз',\n",
       "  'предмет',\n",
       "  'мечта',\n",
       "  'безобразие',\n",
       "  'познание',\n",
       "  'англичанин'],\n",
       " ['якутия',\n",
       "  'книга',\n",
       "  'восстание',\n",
       "  'сибирь',\n",
       "  'леонид',\n",
       "  'красных',\n",
       "  'пустынь',\n",
       "  'перо',\n",
       "  'война',\n",
       "  'мороз']]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.24\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат ещё немного улучшился. Немного подросла точность. Теперь вместо стоп-слов в ключевые попадают имена и все такое. Иногда это хорощо, а иногда нет (собянин - может быть ключевым словом, а дарья - вряд ли)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем этот результат за baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision -  0.13\n",
    "Recall -  0.24\n",
    "F1 -  0.16\n",
    "Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем графы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть методов для извлечения ключевых слов основана на применении графов. Основная идея - каким-то образом перевести текст в граф, а затем каким-то образом расчитать важность каждого узла и вывести топ-N самых важных узлов.  \n",
    "\n",
    "Перевод текста в граф -  не тривиальная задача. Часто применяют такой подход - построим матрицу совстречаемости слов (в каком-то окне), эта матрица будет нашей матрицей смежности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора важных узлов часто используют простой randow walk. Алгоритм примерно такой:  \n",
    "1) Каким-то образом выбирается первый узел графа (например, случайно из равномерного распределения)  \n",
    "2) на основе связей этого узла с другими, выбирается следующий узел  \n",
    "3) шаг два повторяется некоторое количество раз (например, тысячу) __*чтобы не зацикливаться, с какой-то вероятностью мы случайно перескакиваем на другой узел (даже если он никак не связан с текущим, как в шаге 1)__  \n",
    "5) на каждом шаге мы сохраняем узел в котором находимся  \n",
    "6) в конце мы считаем в каких узлах мы были чаще всего и выводим top-N  \n",
    "\n",
    "\n",
    "Предполагается, что мы часто будем приходить в важные узлы графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наглядности реализуем этот подход без networkx. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kws(text, top=5, window_size=5, random_p=0.1):\n",
    "\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    # нормализуем строки, чтобы получилась вероятность перехода\n",
    "    for i in range(m.shape[0]):\n",
    "        m[i] /= np.sum(m[i])\n",
    "    \n",
    "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
    "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
    "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
    "    \n",
    "    c = Counter()\n",
    "    # начнем с абсолютного случайно выбранного элемента\n",
    "    n = np.random.choice(len(vocab))\n",
    "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
    "        \n",
    "        # c вероятностью random_p \n",
    "        # перескакиваем на другой узел\n",
    "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
    "        if go_random:\n",
    "            n = np.random.choice(len(vocab))\n",
    "        \n",
    "        n = take_step(n, m)\n",
    "        # записываем узлы, в которых были\n",
    "        c.update([n])\n",
    "    \n",
    "    # вернем топ-N наиболее часто встретившихся сл\n",
    "    return [id2word[i] for i, count in c.most_common(top)]\n",
    "\n",
    "def take_step(n, matrix):\n",
    "    rang = len(matrix[n])\n",
    "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
    "    next_n = np.random.choice(range(rang), p=matrix[n])\n",
    "    return next_n\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.4 s, sys: 19.3 ms, total: 34.5 s\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keywords_rw = data['content_norm'].apply(lambda x: get_kws(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.11\n",
      "Recall -  0.21\n",
      "F1 -  0.14\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [произведение, предмет, стандарт, реализация, результат, ольга, письмо, фгоса, источник, содержание]\n",
       "1                     [красота, глаз, отчаяние, кошка, дело, руина, предмет, разочарование, мечта, порыв]\n",
       "2                           [пепеляев, книга, юзеф, ночное, мороз, якутия, война, год, сибирь, восстание]\n",
       "3               [команда, пилот, гонка, дженсон, внимание, время, мальдонадо, сезон, испания, реальность]\n",
       "4                                [есенин, поэт, клюев, человек, друг, год, сергей, жизнь, смерть, борода]\n",
       "5            [выпускник, ординатор, специалист, кафедра, работа, практика, медицина, звено, уровень, год]\n",
       "6                  [книга, азербайджан, анатомия, наука, говор, мозг, человек, язык, голов, исследование]\n",
       "7                          [ирак, американец, число, война, нападение, лёт, милиция, мосул, орган, взрыв]\n",
       "8    [нейросеть, клетка, решение, задача, подкрепление, пиксель, фото, время, исследователь, электроника]\n",
       "9    [георгиев, россия, кристалина, слово, директор, организация, вступление, банка, влияние, переговоры]\n",
       "Name: content_norm, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_rw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попбруем теперь важность считать с помощью какой-нибудь метрики из networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(text, window_size=5):\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    return m, id2word\n",
    "\n",
    "def some_centrality_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    # тут можно поставить любую метрику\n",
    "    node2measure = dict(nx.pagerank(G))\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 41 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.24\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keyword_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты не превосходят tfidf, но и не сильно уступают. Явно можно что-то доработать и превзойти baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовое решение есть в gensim. Давайте попробуем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_kws = data['content_norm'].apply(lambda x: keywords(' '.join(x)).split('\\n')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.07\n",
      "Recall -  0.11\n",
      "F1 -  0.08\n",
      "Jaccard -  0.04\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], gensim_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша имплементация отработала получше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре установлен такой бейзлан - F1 -  0.16 (не будем учитывать точность и полноту по отдельности и отбросим жаккара)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ваша задача - предложить 3 способа побить бейзлайн. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет никаких ограничений кроме:\n",
    "\n",
    "1) нельзя изменять метрику\n",
    "2) решение должно быть воспроизводимым"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - https://goo.gl/forms/H9lBH9wCxqq1T0ru2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать мой код как основу, а можно придумать что-то полностью другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 1 балл)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
